A brief essay on [TinyMLOps: Operational Challenges for Widespread Edge AI Adoption](https://arxiv.org/abs/2203.10923), by mtxslv:

Deploying machine learning (ML) applications on edge brings clear benefits such as improved reliability, latency, and privacy. It also introduces its own set of challenges. It is worth knowing that the limited computational resources of edge platforms are not the only bottleneck standing in the way of widespread adoption. Tasks such as monitoring and managing the application (standard functionality for an MLOps platform) are complicated by the distributed nature of edge deployment. Issues unique to edge applications are: protecting a model’s intellectual property and verifying its integrity. These topics will be addressed in this document.

TinyML (the use of ML models on resource-constrained edge devices) is a deployment paradigm for various applications such as smart home appliances, virtual assistants, and autonomous vehicles. Edge deployment provides a set of advantages compared to when the model is evaluated on cloud infrastructure. However, several challenges hinder the large-scale adoption of edge AI (the most obvious is the limited processing power of edge devices). Most of the work in the field of TinyML focuses on improving the efficiency of these models. In this paper, we instead investigate what other challenges arise and what is being done to solve them (on the operational side). Furthermore, we introduce the idea of TinyMLOps, a set of additional considerations that pop up when an ML model is deployed on the edge. 

In the past decade, several ML techniques were applied in various domains. These models are developed by data scientists and the result is a complicated pipeline of scripts and frameworks hard to be deployed in production. Adding ML functionality increases the ways the system can be updated. All these concerns gave rise to the field of MLOps (an extension of DevOps applied to ML). The extension of MLOps is called TinyMLOps.

In a centralized, cloud-based application, it is sufficient to have a single model for all users. On the edge (decentralized), the model will be deployed to the end user’s device. Different devices have different computational resources, storage availability, and network connectivity. Thus, we might need to support multiple models. Existing solutions for storing models, therefore, have to be extended to track the relationship between different versions of the same model.

As ML models are increasingly being used, monitoring is key to ensuring that a model keeps performing as expected. While monitoring on a cloud platform is simpler (the data is centralized), it becomes less trivial if the model is deployed on the edge. Here, stronger privacy guarantees than cloud processing are provided, but we may be interested in recording some telemetry data (execution time, memory, and energy) that signal performance issues. This consideration has to be taken into account when implementing TinyMLOps monitoring. Moreover, since the data cannot be centralized, we rely on Federated Learning (the shared model is an average of several users’ models) to update the model. This is not trivial, since it is difficult to aggregate local, heterogeneous updates. Other issues with this method is the availability of labeled data and computing power. On the other hand, since each model is local, it is possible to use “overfitted” models for each user.

A common business model for ML applications is pay-per-query. This is harder to do in a TinyML situation since the model is running on devices that might not even be connected during inference. A solution to this would be a prepaid package that gives access to a certain number of model calls.

A trained ML model can represent a significant intellectual value for the owner. The reason for this is the highly specialized skill set, time employed and computing resources needed. Then, unscrupulous actors might try to steal the trained model. There are two possible ways: direct and indirect ML model stealing. The former happens when the exact trained weights are obtained. The latter describes the practice of using the model to easily label data to be used to train another model. There are several protection mechanisms against these attacks, like encryption, watermarking, and prediction poisoning. Moreover, since the ML model is only a small part of the pipeline (whose predictions trigger other actions), its predictions can be altered to trick the system. A verifiable computation component may ascertain the ML result is trustworthy. All these techniques discussed are not trivial to implement securely. A TinyMLOps platform that automatically generates verifiable modules would be of great value.

